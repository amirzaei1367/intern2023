from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
import nltk
from sklearn.metrics.pairwise import cosine_similarity
import gensim.downloader as api
import pandas as pd
import os

nltk.download('punkt')  # Download tokenizer data for NLTK

# Load pre-trained GloVe word embeddings
glove_model = api.load("glove-wiki-gigaword-100")  # You can choose different dimensions (e.g., glove-wiki-gigaword-50)

# Folder path where the PNG images are stored
image_folder_path = "/Users/montrellnelson/Downloads/REI_IMAGES/"

# CSV file path containing serial numbers and corresponding ground truth captions
csv_file_path = "/Users/montrellnelson/Downloads/df_design_search_1000.csv"

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")

# Read the CSV file to get the serial numbers and ground truth captions
df = pd.read_csv(csv_file_path)

total_cos_similarity = 0
cos_similarity_list = []
num_pairs = len(df)

ground_truth_captions_dict = {}


# Dictionary to store generated captions for each image
generated_captions_dict = {}


for index, row in df.iterrows():
    serial_number = row['serial_no']
    ground_truth_caption = row['td_desc']

    # Load the image from the folder using the serial number as the image file name
    image_file_name = f"{serial_number}.jpg"  # Assuming the images have the ".jpg" extension
    image_path = os.path.join(image_folder_path, image_file_name)
    raw_image = Image.open(image_path).convert('RGB')

    # Conditional image captioning
    text = ""
    inputs = processor(raw_image, text, return_tensors="pt")

    out = model.generate(**inputs)  # Generate a new caption for each image
    generated_caption = processor.decode(out[0], skip_special_tokens=True)



    # Tokenize the generated and ground truth captions
    generated_caption_tokens = nltk.word_tokenize(generated_caption)
    ground_truth_tokens = nltk.word_tokenize(ground_truth_caption)

    # Get the word embeddings for each word in the captions (if available)
    generated_caption_vectors = [glove_model[word] for word in generated_caption_tokens if word in glove_model]
    ground_truth_vectors = [glove_model[word] for word in ground_truth_tokens if word in glove_model]

    # Calculate the average word embeddings to get the vector representations of the captions
    generated_caption_vector = sum(generated_caption_vectors) / len(generated_caption_vectors) if generated_caption_vectors else None
    ground_truth_vector = sum(ground_truth_vectors) / len(ground_truth_vectors) if ground_truth_vectors else None

    # Calculate Cosine Similarity
    cos_similarity = cosine_similarity([generated_caption_vector], [ground_truth_vector])[0][0] if generated_caption_vector is not None and ground_truth_vector is not None else None

    total_cos_similarity += cos_similarity if cos_similarity is not None else 0
    cos_similarity_list.append((cos_similarity, serial_number))

# Store the generated caption in the dictionary
    generated_captions_dict[serial_number] = generated_caption

# Store the ground truth caption in the dictionary
    ground_truth_captions_dict[serial_number] = ground_truth_caption
    
    # print("Cosine Similarity:", cos_similarity)
    # print("Ground Truth Caption:", ground_truth_caption)
    # print("Generated Caption:", generated_caption)


#     print("-" * 50)
